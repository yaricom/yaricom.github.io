<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>IO42 Learning to Learn on IO42 Learning to Learn</title>
    <link>/</link>
    <description>Recent content in IO42 Learning to Learn on IO42 Learning to Learn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Iaroslav Omelianenko</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0300</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Self Replication to Preserve Innate Learned Structures in Artificial Neural Networks</title>
      <link>/post/self-replication-to-preserve-innate-learned-structures-in-ann/</link>
      <pubDate>Fri, 27 Jul 2018 18:57:35 +0300</pubDate>
      
      <guid>/post/self-replication-to-preserve-innate-learned-structures-in-ann/</guid>
      <description>

&lt;p&gt;It’s interesting to investigate combination of deep &lt;a href=&#34;https://medium.com/@io42/neuroevolution-evolving-artificial-neural-networks-topology-from-the-scratch-d1ebc5540d84&#34; target=&#34;_blank&#34;&gt;neuro-evolution&lt;/a&gt; and self-replication&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; to evolve Artificial Neural Networks (ANNs) able to keep and complexify innate learned structures aimed to fulfill auxiliary tasks (orthogonal to the self-replication).&lt;/p&gt;

&lt;p&gt;In such a way, it may became possible to build &lt;em&gt;evolutionary lineage tree&lt;/em&gt; of ANNs specialized to complete specific tasks under different environmental settings through subsequent training sessions. And the knowledge acquired during all these training sessions will accumulate not only in the form of learned connections’ weights, but in a neural network’s &lt;em&gt;topology&lt;/em&gt; as well.&lt;/p&gt;

&lt;p&gt;Subsequently, with appropriate orchestration of resulting intelligent agents it can be possible to produce swarm intelligence with much higher order of complexity than of each individual agent, able to adequately solve imperfect knowledge problems through weighted consensus among all participants.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Biological life began with the first self-replicator, and natural selection kicked in to favor organisms that are better at replication, resulting in a self- improving mechanism. Analogously, we can construct a self-improving mechanism for artificial intelligence via natural selection, if AI agents had the ability to replicate and improve themselves without additional machinery.&lt;/em&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Numerous recent studies have shown that various brain structures include &lt;em&gt;innate knowledge seeds&lt;/em&gt; that explode into inalienable abilities of living organisms to learn or complete specific tasks right from the first seconds of immediate life experience.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Over 90% of our genes are expressed in the development of the brain, and a significant number of those are expressed selectively, in a way that allows the brain to self-assemble, even, to some non-trivial degree in the absence of experience. Mechanisms such as cell division, cell differentiation, cell migration, cell death, and axon guidance combine to self-assemble a rich first draft of the human brain, even prior to experience. Even in the absence of synaptic transmission, the primary mechanism by which experience is conveyed to the brain, the basic structure of the newborn brain is preserved.&lt;/em&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This demonstrates that different brain structures can be regarded to some extent as a swarm of intelligent agents that effectively combine weighted responses from different regions, from cortex to amygdala, in order to build an adequate complex response on the external stimuli created by the environment. It is also worth noting that different human brain structures was inherited from earlier inhuman forms of life, and we even have brain structures related to prehistoric reptiles. Thus, the development of innate brain structures can be regarded as a long-term evolutionary process of complexification from the most basic structures controlling autonomic peripheral neural system to the most complex structures supporting abstract reasoning. Such kind of increasing complexification allows evolution not to get stuck on local optima and to develop even more complex self-organizing structures through &lt;em&gt;dissipative adaptation&lt;/em&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Life manages to squeeze exquisite reliability in behaviour on large scales from a jittery herd of individual molecules, without always needing to put each atom in its place, and we might therefore feel encouraged to attempt something similar in our own feats of engineering.&lt;/em&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This point of view allows us to consider the evolution of synthetic intelligence in a similar way. The main idea is to simulate the process of natural selection that influences evolution of the life forms, by creating of specific &lt;em&gt;multi-staged environment&lt;/em&gt; that force neuro-evolution to generate highly specialized ANNs that can survive as a swarm. The training multi-stage environment must produce multiple challenges with increasing survival pressure. The key point here is the creation at each stage of an environmental challenge with ever increasing amount of available training signals. This will allow to evolve intelligent systems with innate knowledge of environment from it’s most basic form to a comprehensive understanding of the whole. The search for an evolutionary champion at each training stage can be carried out using a &lt;em&gt;novelty search&lt;/em&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; method that allows to explore all available survival options and to find the best fit among them.&lt;/p&gt;

&lt;p&gt;By combining orthogonal goals such as self-replication and survival in an increasingly complex environment, it’s possible to create adaptive pressure on the ANN’s evolutionary tree that will ignite the generation of complex systems with innate structures, capable to quickly learn new properties of the environment from the first moments of direct experience. As an added bonus, it will also sustain life-long learning of produced intelligent systems, by effectively incorporating new experiences into innate structures.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Self-replication involves a degree of introspection and self-awareness, which is helpful for lifelong learning and potential discovery of new neural network architectures.&lt;/em&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The resulting swarm of intelligent agents can become a foundation for creation of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_general_intelligence&#34; target=&#34;_blank&#34;&gt;Artificial General Intelligence&lt;/a&gt; (AGI) with innate structures that allow to generalize knowledge about new experiences and environmental challenges.&lt;/p&gt;

&lt;p&gt;~&lt;/p&gt;

&lt;p&gt;The repost of the original article posted by author at Medium: &lt;a href=&#34;https://medium.com/@io42/self-replication-to-preserve-innate-leaned-structures-in-artificial-neural-networks-9bd8758662b4&#34; target=&#34;_blank&#34;&gt;Self-replication to preserve innate learned structures in Artificial Neural Networks&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;references-1-2-3-4-5-6-7&#34;&gt;References: &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Oscar Chang, Hod Lipson, &lt;a href=&#34;https://arxiv.org/abs/1803.05859&#34; target=&#34;_blank&#34;&gt;Neural Network Quine&lt;/a&gt;, arXiv preprint: 1803.05859v3, 2018
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Gary Marcus, &lt;a href=&#34;https://arxiv.org/abs/1801.05667&#34; target=&#34;_blank&#34;&gt;Innateness, AlphaZero, and Artificial Intelligence&lt;/a&gt;, arXiv preprint: 1801.05667v1, 2018
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Jeremy L. England, &lt;a href=&#34;https://www.englandlab.com/uploads/7/8/0/3/7803054/nnano.2015.250__1_.pdf&#34; target=&#34;_blank&#34;&gt;Dissipative adaptation in driven self-assembly&lt;/a&gt;, Nature Nanotechnology volume 10, pages 919–923, 2015
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;Joel Lehman, &lt;a href=&#34;http://joellehman.com/lehman-dissertation.pdf&#34; target=&#34;_blank&#34;&gt;Evolution through the search for novelty&lt;/a&gt;, B.S. Ohio State University, 2007
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;Robert Marsland III, Jeremy L. England, &lt;a href=&#34;https://arxiv.org/abs/1711.02172&#34; target=&#34;_blank&#34;&gt;Speed, strength and dissipation in biological self-assembly&lt;/a&gt;, arXiv preprint: 1711.02172v1, 2017
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;Sumantra Sarkar, Bin Wang, Jeremy L. England, &lt;a href=&#34;https://arxiv.org/abs/1709.09191&#34; target=&#34;_blank&#34;&gt;Design of conditions for emergence of self-replicators&lt;/a&gt;, arXiv preprint: 1709.09191v2, 2018
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:6&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;Iaroslav Omelianenko, &lt;a href=&#34;https://medium.com/@io42/neuroevolution-evolving-artificial-neural-networks-topology-from-the-scratch-d1ebc5540d84&#34; target=&#34;_blank&#34;&gt;Neuroevolution — evolving Artificial Neural Networks topology from the scratch&lt;/a&gt;, Medium, 2018
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:7&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GoNEAT_NS</title>
      <link>/project/goneat_ns/</link>
      <pubDate>Wed, 25 Jul 2018 18:30:58 +0300</pubDate>
      
      <guid>/project/goneat_ns/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Go NEAT</title>
      <link>/project/goneat/</link>
      <pubDate>Wed, 25 Jul 2018 17:55:31 +0300</pubDate>
      
      <guid>/project/goneat/</guid>
      <description>&lt;p&gt;This project provides implementation of &lt;a href=&#34;http://www.cs.ucf.edu/%7Ekstanley/neat.html&#34; target=&#34;_blank&#34;&gt;NeuroEvolution of Augmenting Topologies&lt;/a&gt; (NEAT) method written in Go language.&lt;/p&gt;

&lt;p&gt;The Neuroevolution (NE) is an artificial evolution of Neural Networks (NN) using genetic algorithms in order to find optimal NN parameters and topology. Neuroevolution of NN may assume search for optimal weights of connections between NN nodes as well as search for optimal topology of resulting NN. The NEAT method implemented in this work do search for both: optimal connections weights and topology for given task (number of NN nodes per layer and their interconnections).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FTC 2017</title>
      <link>/talk/ftc2017/</link>
      <pubDate>Wed, 25 Jul 2018 14:51:50 +0300</pubDate>
      
      <guid>/talk/ftc2017/</guid>
      <description>&lt;p&gt;The full reference of the presented work is:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Omelianenko, I (2017). Applying Deep Machine Learning for Psycho-Demographic Profiling of Internet Users using O.C.E.A.N. Model of Personality. Proceedings of the 2017 Future Technologies Conference (SAI) – IEEE, Vancouver, Canada, ISBN (USB) 978-1-5386-1744-1, pp. 375-384&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Neuroevolution - Evolving Artificial Neural Networks Topology From the Scratch</title>
      <link>/post/neuroevolution-evolving-ann-topology-from-the-scratch/</link>
      <pubDate>Wed, 25 Jul 2018 13:55:17 +0300</pubDate>
      
      <guid>/post/neuroevolution-evolving-ann-topology-from-the-scratch/</guid>
      <description>&lt;p&gt;The most popular method of &lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_neural_network&#34; target=&#34;_blank&#34;&gt;Artificial Neural Networks&lt;/a&gt; (ANN) training -at the time of this essay writing - is to use some form of Gradient Descent (GD) combined with error back propagation w.r.t. objective function defining our learning goal. This methodology was invented about 30 years ago by &lt;a href=&#34;https://en.wikipedia.org/wiki/Geoffrey_Hinton&#34; target=&#34;_blank&#34;&gt;Geoffrey Hinton&lt;/a&gt; and become a foundation of all modern research activities in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Deep_learning&#34; target=&#34;_blank&#34;&gt;Deep Machine Learning&lt;/a&gt; (ML) and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_intelligence&#34; target=&#34;_blank&#34;&gt;Artificial Intelligence&lt;/a&gt; (AI). But despite the fact that it gives immense power in areas of pattern recognition (&lt;a href=&#34;https://en.wikipedia.org/wiki/Feature_learning&#34; target=&#34;_blank&#34;&gt;feature or representation learning&lt;/a&gt;) it has considerable weakness:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the network topology must be fully hand-engineered before training starts and only connection weights encapsulate learned knowledge, the network topology remain the same&lt;/li&gt;
&lt;li&gt;as a result it may introduce oversatturated neural units which don not take part in the training/inference process but simply consuming comptunig resources&lt;/li&gt;
&lt;li&gt;special methodic must be applied to avoid sticking into local optima such as L1/L2-norm, dropout regularizations, etc&lt;/li&gt;
&lt;li&gt;exploding or diminishing learning gradient issues during back/forward propagation&lt;/li&gt;
&lt;li&gt;implemented solutions can generalize only in the narrow scope learned from provided training samples&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;To overcome some of the drawbacks of GD-based training it was proposed to use alternative methods to train/evolve neural networks with group of algorithms inspired by natural selection and genetic evolution. It was given name &lt;a href=&#34;https://en.wikipedia.org/wiki/Genetic_algorithm&#34; target=&#34;_blank&#34;&gt;Genetic Algorithms&lt;/a&gt; (GA) to address the source of inspiration and due to its attempt to mimic natural process of genetic mutations, crossover and selection, while trying to solve objective function optimization problem. There are many types of such algorithms was invented during last years.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/img/posts/1/crossover_mutation.png&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;The example of Crossover and Mutation in GA (&lt;a href=&#34;http://www.abrandao.com/2015/01/simple-php-genetic-algorithm/&#34; target=&#34;_blank&#34;&gt;image source&lt;/a&gt;)&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;But my main interest in this essay is to present a kind of GA which can be applied to evolve ANNs from the most simple forms to the complex ones in order to find objective function optimization solution not only by changing weights of connections between neural units, but by evolving the topology of network graph itself.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I would like to consider &lt;a href=&#34;http://www.cs.ucf.edu/~kstanley/neat.html&#34; target=&#34;_blank&#34;&gt;Neuroevolution of Augmented Topologies&lt;/a&gt; (NEAT) algorithm invented by &lt;a href=&#34;http://www.cs.ucf.edu/~kstanley/&#34; target=&#34;_blank&#34;&gt;Kenneth O. Stanley&lt;/a&gt; as part of his &lt;a href=&#34;http://nn.cs.utexas.edu/keyword?stanley:phd04&#34; target=&#34;_blank&#34;&gt;Phd Thesis&lt;/a&gt; in years 2002-2004. With this method of ANN evolution, search for complex solutions made feasible through graduate complexification of network topology. By starting with minimal ANN the NEAT is more likely to find efficient and robust solution, avoiding sticking at the local optima as in cases with other GA methods which starts with elaborated network graphs and mutate them during training.&lt;/p&gt;

&lt;p&gt;With NEAT method, the training starts with very simple ANNs topology comprising of only input, output and bias neural units - no hidden units introduced at the beginning. Thus it ensures, that the system searches for the solution in the lowest-dimensional weight space possible over the course of all generations. The goal is not to minimize only final product, but all intermediate networks along the way as well. This idea is they key to gaining an advantage from the evolution of topology: it allows us to minimize the search space, resulting in dramatic performance gains.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/img/posts/1/genotype-phenotype-mapping.png&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;A genotype to phenotype mapping example. A genotype is depicted that produces the shown phenotype.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;There are two main types of structural mutations present in the NEAT algorithm: adding the connection between nodes or adding the new node. When mutation is performed, the new added gene (connection gene or node gene) will be assigned with increasingly incremented &lt;em&gt;innovation number&lt;/em&gt;.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/img/posts/1/types-of-structural-mutations.png&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;In adding a connection, a single new connection gene is added to the end of the genome and given the next available innovation number. In adding a new node, the connection gene being split is disabled, and two new connection genes are added to the end the genome. The new node is between the two new connections.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Through mutation, the genomes in NEAT will gradually get larger. Genomes of varying sizes will result, sometimes with different connections (genes) at the same positions.&lt;/p&gt;

&lt;p&gt;There is an unexploited information in evolution, that tells us exactly which genes match up with which genes between &lt;em&gt;any&lt;/em&gt; individuals in a topologically diverse population. That information is the historical origin of each gene. Two genes with the same historical origin must represent the same structure (although possibly with different weights), since they are both derived from the same ancestral gene of some point in the past. Thus, all the system needs to do, in order to know which genes line up with which, is to keep track of the historical origin of every gene in the population’s genome.&lt;/p&gt;

&lt;p&gt;Luckily for us, the &lt;em&gt;innovation numbers&lt;/em&gt; incrementally assigned to the genes during genome mutations is a kind of &lt;em&gt;historical markers&lt;/em&gt; to use for tracking chronology of structural genome mutations. At the same time, during crossover (mating), the offsprings will inherit the same innovation numbers of genes in the parents genome. Thus, innovation number of particular gene will never change, allowing tracking of historical origin of every gene throughout evolution.&lt;/p&gt;

&lt;p&gt;The historical markers give NEAT a power to track which genes match up with which. Thus, during the crossover, system will know exactly how to lineup genes from genomes of both parents. The genes with matching innovation numbers will be called &lt;em&gt;matching&lt;/em&gt; genes. Genes that do not match are either &lt;em&gt;disjoint&lt;/em&gt; or &lt;em&gt;excess&lt;/em&gt;, depending on whether they occur within or outside the range of the other parent’s innovation numbers. They represent structure that is not present in the other parent’s genome. When composing the offspring, genes are randomly chosen from either parent at matching genes, whereas all excess or disjoint genes are always included from the more fit parent. This way, historical markings allow NEAT to perform crossover using linear genomes encoding without the need for expensive topological analysis.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/img/posts/1/crossover-using-linear-genomes-encoding.png&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;During the crossover, the offspring genes are randomly chosen from matching genes of either parent and disjoint/excess genes taken from most fit parent. All diagrams from original &lt;a href=&#34;http://nn.cs.utexas.edu/?stanley:gecco02b&#34; target=&#34;_blank&#34;&gt;NEAT paper&lt;/a&gt;, highly recommended reading!&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Using proposed method the population of organisms can evolve diverse topologies, but it happens that such population can not evolve and maintain &lt;em&gt;topological innovations&lt;/em&gt; on its own. The smaller structures optimize faster than larger structures. Thus by adding new nodes and connections to some topology we artificially reduce it’s chances for survival. The freshly augmented topologies usually experience initial decrease in the fitness, even though the innovations they represent may be resulting in winning solution in the long run.&lt;/p&gt;

&lt;p&gt;This can be solved by introducing &lt;em&gt;speciation&lt;/em&gt; to the population which additionally limits range of organisms that can mate. With speciation it’s possible to organize crossover in such a way that organisms will compete only in narrow niches instead of all population in general. The idea is to divide the population such that similar topologies are in the same species.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;With all specific tweaks to general GA introduced with NEAT it&amp;rsquo;s possible to build complex ANNs to solve control optimization problems among other unsupervised learning problems. Due to specifics of ANN topology augmentation through complexification and speciation found solutions tends to be performance optimized from the train as well as from the inference point of view. The resulting ANNs topology grows exactly to match problem to be solved without any excess layers of hidden units introduced with traditional approach of ANN&amp;rsquo;s topology hand-engineering.&lt;/p&gt;

&lt;p&gt;Due to this it&amp;rsquo;s possible to build complex ensembles of specific ANNs to solve most complex problems arising when attempting to build AI systems. Such ensembles of highly specialized small ANNs can be combined in a way as neural networks combined in the human brain, where each specific part responsible for the processing of particular stimulus or specialized activity.&lt;/p&gt;

&lt;p&gt;Another application of ANNs ensembles is to create solvers for imperfect information games by applying sub-game solving strategy proposed in this &lt;a href=&#34;https://arxiv.org/abs/1705.02955&#34; target=&#34;_blank&#34;&gt;research paper&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;There are exists number of implementations of NEAT algorithm in &lt;a href=&#34;http://eplex.cs.ucf.edu/neat_software/&#34; target=&#34;_blank&#34;&gt;diverse programming languages&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The author also provided his own implementation of NEAT in GO programming language with extended verification benchmarks: XOR, single-, and double-pole balancing. The source code of the implementation is available at GitHub: &lt;a href=&#34;https://github.com/yaricom/goNEAT&#34; target=&#34;_blank&#34;&gt;https://github.com/yaricom/goNEAT&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The associated research project can be found at ResearchGate: &lt;a href=&#34;https://www.researchgate.net/project/NeuroEvolution-of-Augmented-Topologies&#34; target=&#34;_blank&#34;&gt;https://www.researchgate.net/project/NeuroEvolution-of-Augmented-Topologies&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;~&lt;/p&gt;

&lt;p&gt;The repost of the original article posted by author at Medium: &lt;a href=&#34;https://medium.com/@io42/neuroevolution-evolving-artificial-neural-networks-topology-from-the-scratch-d1ebc5540d84&#34; target=&#34;_blank&#34;&gt;Neuroevolution - evolving Artificial Neural Networks topology from the scratch&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FTC2017</title>
      <link>/publication/ftc2017/</link>
      <pubDate>Wed, 29 Nov 2017 13:30:00 -0800</pubDate>
      
      <guid>/publication/ftc2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SSVEP Brain Hash Function</title>
      <link>/project/brainhash/</link>
      <pubDate>Thu, 03 Aug 2017 18:25:34 +0300</pubDate>
      
      <guid>/project/brainhash/</guid>
      <description>&lt;p&gt;This project had a goal to research if steady-state visually evoked potential (SSVEP) can be used to create Brain Hash Function Algorithm able to distinguish between unique footprints of each individual brain under specific visual stimulation with electro-physiological visual feedback based on consumer-grade EEG monitoring device.&lt;/p&gt;

&lt;p&gt;The project scope consist of two major parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Implementation of electro-physiological visual feedback system based on consumer-grade EEG monitoring device. It should perform monitoring of EEG signals in real time and perform preprocessing of received raw EEG signal.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Implementation of advance machine-learning pipeline to automatically extract important features from data stream and perform classification of encoded features. It should perform confident classification of the collected EEG data in order to (a) reliably distinguish signal from noise and (b) reliably distinguish between EEG records collected from different human participants.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The project timeline and results can be found at: &lt;a href=&#34;https://www.researchgate.net/project/Brain-Hash-Function&#34; target=&#34;_blank&#34;&gt;ResearchGate&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>arXiv:1708.01167</title>
      <link>/publication/arxiv-1708.01167/</link>
      <pubDate>Thu, 03 Aug 2017 17:32:12 +0300</pubDate>
      
      <guid>/publication/arxiv-1708.01167/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Psistats</title>
      <link>/project/psistats/</link>
      <pubDate>Wed, 05 Jul 2017 19:10:25 +0300</pubDate>
      
      <guid>/project/psistats/</guid>
      <description>&lt;p&gt;The project had a goal to provide working implementation of psycho-demographic profiling algorithm which can be used to profile Internet users based on digital footprints they leave by using various Internet services. We provide full source code implementation in R programming language of the algorithms described in corresponding research paper.&lt;/p&gt;

&lt;p&gt;We have used variety of machine learning and data mining tools in order to process input data corpus and to create artificial neural networks for multivariate regression analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>arXiv:1703.06914</title>
      <link>/publication/arxiv-1703.06914/</link>
      <pubDate>Tue, 07 Mar 2017 12:27:21 +0300</pubDate>
      
      <guid>/publication/arxiv-1703.06914/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
